Some steps to take before getting smart actually working with mpich2-yarn:

1. Need to set some settings in mpi-site.xml in example_configuration/ folder:

Set yarn.mpi.scratch.dir value to hdfs://10.1.255.126:8020/nn/mpi-tmp (hdfs namenode)
Set yarn.mpi.ssh.authorizedkeys.path to /home/yarn/.ssh/authorized_keys

Also create the kown_hosts file

2. Copy the mpi-site.xml to /etc/hadoop/conf/
3. export HADOOP_USER_NAME = hdfs (not sure if a better way does not exist)
4. Make changes to MPIConfiguration.java file
  set  DEFAULT_MPI_SCRATCH_DIR to hdfs://10.1.255.126:8020/group/dc/mpi-tmp (not sure?)
  set DEFAULT_MPI_SSH_PUBLICKEY_ADDR to /home/hdfs/.ssh/id_rsa.pub (not sure?) 
5. Create /tmp/hadoop-yarn, owned by user yarn. 


If other user is going to submit jobs (other than yarn / hdfs), we need to do the
following on all nodes (for example when using LinuxContainerExecutor):

As user hdfs run:
$hdfs dfs -chown -R hdfs:hadoop /nn
$hdfs dfs -chmod -R 664 /nn
$hdfs dfs -chmod -R +X /nn
As root
$usermod -a -G hadoop <the user to run yarn job>

Also do the following:
$mkdir /tmp/hadoop-<the user> , where <the user> could be a username such as mrashti.
$sudo chown <the user>:hadoop /tmp/hadoop-<the user>
- create /home/<the user>/.ssh/authorized_keys
$export HADOOP_USER_NAME=<the user>  
- in mpi-site.xml, set yarn.mpi.ssh.authorizedkeys.path to /home/<the user>/.ssh/authorized_keys

Also make sure mpich2 is in the path:
export PATH=/opt/mpich-3.1.2_install/bin/:$PATH

And here is an example of running logstic regression in Smart over Yarn:
(/data/home/mrashti/projects/informer_hpcc/Smart/examples/offline_analytics)

yarn jar ../../../mpich2-yarn/target/mpich2-yarn-1.0-SNAPSHOT.jar -a ./logistic_regression_offline -M 1024 -m 1024 -n 2 -c 2

If an error occurs and application does not finish successfully, inspect Hadoop logs at:

http://compute-1-1.local:8088/cluster

