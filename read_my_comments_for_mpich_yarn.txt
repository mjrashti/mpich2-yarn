Some steps to take before getting this actually working:

1. Need to set some settings in mpi-site.xml in example_configuration/ folder:

Set yarn.mpi.scratch.dir value to hdfs://10.1.255.126:8020/nn/mpi-tmp (hdfs namenode)
Set yarn.mpi.ssh.authorizedkeys.path to /home/yarn/.ssh/authorized_keys

Also create the kown_hosts file

2. Copy the mpi-site.xml to /etc/hadoop/conf/
3. export HADOOP_USER_NAME = hdfs (not sure if a better way does not exist)
4. Make changes to MPIConfiguration.java file
  set  DEFAULT_MPI_SCRATCH_DIR to hdfs://10.1.255.126:8020/group/dc/mpi-tmp (not sure?)
  set DEFAULT_MPI_SSH_PUBLICKEY_ADDR to /home/hdfs/.ssh/id_rsa.pub (not sure?) 
5. Create /tmp/hadoop-yarn, owned by user yarn. 


If other user is going to submit jobs (other than yarn / hdfs), we need to do the
following on all nodes (for example when using LinuxContainerExecutor):

As user hdfs run:
$hdfs dfs -chown -R hdfs:hadoop /nn
$hdfs dfs -chmod -R 664 /nn
$hdfs dfs -chmod -R +X /nn
As root
$usermod -a -G hadoop <the user to run yarn job>

Also do the following:
$mkdir /tmp/hadoop-<the user>
$sudo chown mrashti:hadoop /tmp/hadoop-mrashti
- create /home/<the user>/.ssh/authorized_keys
$export HADOOP_USER_NAME=<the user>  
- in mpi-site.xml, set yarn.mpi.ssh.authorizedkeys.path to /home/<the user>/.ssh/authorized_keys

